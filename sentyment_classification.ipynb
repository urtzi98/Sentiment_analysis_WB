{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "from numpy.core.fromnumeric import mean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, LSTM, GRU, Bidirectional, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "In the following cell we are defining the constants for the dataset csv and the paths where the best weights for each model will be saved. The function creates a model_checkpoint to store the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "DATA_PATH = \"IMDB Dataset.csv\"\n",
    "MAX_LEN = 100\n",
    "checkpoint_filepath_lstm = 'checkpoint/weights_lstm'\n",
    "checkpoint_filepath_cnn = 'checkpoint/weights_cnn'\n",
    "checkpoint_filepath_bi_lstm = 'checkpoint/weights_bi_lstm'\n",
    "checkpoint_filepath_gru = 'checkpoint/weights_gru'\n",
    "checkpoint_filepath_best_lstm = 'checkpoint/weights_best_lstm'\n",
    "checkpoint_filepath_simple_nn = 'checkpoint/weights_simple_nn'\n",
    "\n",
    "def create_checkpoint(path):\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath = path,\n",
    "        save_weights_only = True,\n",
    "        monitor = 'val_acc',\n",
    "        mode = 'max',\n",
    "        save_best_only = True\n",
    "    )\n",
    "    \n",
    "    return model_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the dataset\n",
    "The file contains 50,000 records and two columns: review and sentiment. The review column contains text for the review and the sentiment column contains sentiment for the review. The sentiment column can have two values:\n",
    "- Positive\n",
    "- Negative\n",
    "\n",
    "It is a binary classification problem and a supervised problem because the class classification is known beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    # Kaggle dataset of 50K Moview Reviews\n",
    "    movie_reviews = pd.read_csv(path)\n",
    "    movie_reviews.isnull().values.any()\n",
    "    \n",
    "    return movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data:\n",
    "The reviews contain punctuations, brackets and some HTML tags and all of them must be reviewed for a good training. The function that preprocesses the data takes as input parameter a review in form of string and removes special characters and HTM tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(review):\n",
    "    review = remove_tags(review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', review)\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test:\n",
    "The following function takes as input argument the panda data frame that is the csv file, this data frame has two columns:\n",
    "1. For the first column, each review is preprocessed and stored in a list.\n",
    "2. For the second column, the class is binarized and stored in a list.\n",
    "\n",
    "Finally, we have the X array that is the input data with what the model will train and the array y that is classification class binarized.\n",
    "\n",
    "We will split the data into train and test subsets using the scikit learn train_test_split function, with a test size of 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(movie_reviews):\n",
    "    X = []\n",
    "    reviews = list(movie_reviews['review'])\n",
    "    for review in reviews:\n",
    "        X.append(preprocess_text(review))\n",
    "    y = movie_reviews['sentiment']\n",
    "    y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "First step is to create the tokenizer model to create a word-to-index dictionary where each word is used as a key and a unique index is used as the value for the key.\n",
    "\n",
    "Second step is to use the GloVe embeddings file to create the feature matrix:\n",
    "- File accessible from Kaggle: https://www.kaggle.com/danielwillgeorge/glove6b100dtxt\n",
    "\n",
    "By loading the GloVe embeddings file we will create a dictinary with words as keys and their corresponding embedding list as values.\n",
    "\n",
    "Finally, we will create an embedding matrix where each row number will correspond to the index of the word in the corpus. The matrix will have 100 columns where each column will contain the GloVe word embeddings for the words in our corpus. The number of columns can be changed depending on what we want but for this purpose it is correct to set it to 100, if one row has less than 100 columns the missing ones will be filled with zeros.\n",
    "\n",
    "Embedding_matrix = 92547x100, so the corpus is formed by 92547 words. The embedding matrix will be the input weigts for training the models so it is very important to build it correctly\n",
    "\n",
    "**Note:** The GloVe file must be placed in the same directory in which it is this notebook for the functions to work, and it is necessary to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embedding_layer(X_train, X_test):\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # unique words of the corpus\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=MAX_LEN)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=MAX_LEN)\n",
    "\n",
    "    embeddings_dictionary = dict()\n",
    "    glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "    return X_train, X_test, embedding_matrix, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing graphs\n",
    "This function takes as input parameter the history of the trained model and prints two different graphs:\n",
    "1. Accuracy: comparison between the accuracy in validation and test\n",
    "2. Loss: comparison between the loss in validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graphs(out_values):\n",
    "    plt.plot(out_values.history['acc'])\n",
    "    plt.plot(out_values.history['val_acc'])\n",
    "\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc = 'upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(out_values.history['loss'])\n",
    "    plt.plot(out_values.history['val_loss'])\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc = 'upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we provide 2 functions. The first one to plot the ROC curve for a given TPR (True Positive Rate) and FNR (False Negative Rate). The second one plots a confusion matrix provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ROC_curve(fpr, tpr, roc_auc):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "This function takes as input the predictions and the real outputs and calculates returns some metric values to measure the performace of the model. This metrics are: TPD, FRP and AUC to calculate the ROC curve; as well as the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(y_true, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    print (fpr, tpr)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    c = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return fpr, tpr, roc_auc, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "We also privide a function that, given a model and a concrete review, predicts if that review is a positive or negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, path, rvw):\n",
    "    model.load_weights(path)\n",
    "    rvw = tokenizer.texts_to_sequences(rvw)\n",
    "    #padding the review to have exactly the same shape as `embedding_2` input\n",
    "    rvw = pad_sequences(rvw, maxlen=MAX_LEN, dtype='int32', value=0)\n",
    "    sentiment = model.predict(rvw,batch_size=1,verbose = 2)[0]\n",
    "    print(sentiment)\n",
    "    if sentiment <= 0.5:\n",
    "        print(\"The review is negative\")\n",
    "    else:\n",
    "        print(\"The review is positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step: Generate the dataset\n",
    "\n",
    "Once all the reqired functions are impelemnted and before start to analize some models, we are going to obtain the data. To do so:\n",
    "\n",
    "1. We read the dataset.\n",
    "2. Create the train and test datasets.\n",
    "3. Generate the embeddings of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and split into train and test\n",
    "movie_reviews = read_data(DATA_PATH)\n",
    "X_train, X_test, y_train, y_test = create_train_test(movie_reviews)\n",
    "X_train, X_test, embedding_matrix, vocab_size, tokenizer= prepare_embedding_layer(X_train, X_test) # The reviews are processed as numeric lists of size 100 following a corpus\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_nn(vocab_size, embedding_matrix):\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(vocab_size, input_dim=100, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_nn(X_train, y_train, X_test, y_test, model):\n",
    "    model_checkpoint_callback_simple_nn = create_checkpoint(checkpoint_filepath_simple_nn)\n",
    "    out_values = model.fit(X_train, y_train, batch_size=128, epochs=6,\n",
    "                           callbacks=[model_checkpoint_callback_simple_nn],\n",
    "                           verbose=1, validation_split=0.2)\n",
    "\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Test Score for the simple nn model:\", score_test[0])\n",
    "    print(\"Test Accuracy for the simple nn model:\", score_test[1])\n",
    "    print_graphs(out_values)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_simple_nn(vocab_size, embedding_matrix)\n",
    "print(model.summary())\n",
    "model = train_simple_nn(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for the simple NN model\n",
    "model = create_simple_nn(vocab_size, embedding_matrix)\n",
    "model.load_weights(checkpoint_filepath_simple_nn)\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "\n",
    "fpr, tpr, roc_auc, c = get_performance_metrics(y_test.reshape(-1, 1), predictions.astype('int'))\n",
    "print_ROC_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "plot_confusion_matrix(c, normalize=True, target_names=['Negative Review', 'Positive Review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru(vocab_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=MAX_LEN , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU (units = 64, return_sequences = True,\n",
    "                input_shape = [X_train.shape[0], X_train.shape[0]]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU (units = 64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 1))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gru(X_train, y_train, X_test, y_test, model):\n",
    "    model_checkpoint_callback_gru = create_checkpoint(checkpoint_filepath_gru)\n",
    "    out_values = model.fit(X_train, y_train, batch_size=128, epochs=6,\n",
    "                           callbacks=[model_checkpoint_callback_gru],\n",
    "                           verbose=1, validation_split=0.2)\n",
    "\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Test Score for the GRU model:\", score_test[0])\n",
    "    print(\"Test Accuracy for the GRU model:\", score_test[1])\n",
    "    print_graphs(out_values)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gru model\n",
    "model = create_gru(vocab_size, embedding_matrix)\n",
    "print(model.summary())\n",
    "model = train_gru(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for the Gru model\n",
    "model = create_gru(vocab_size, embedding_matrix)\n",
    "model.load_weights(checkpoint_filepath_gru)\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "\n",
    "fpr, tpr, roc_auc, c = get_performance_metrics(y_test.reshape(-1, 1), predictions.astype('int'))\n",
    "print_ROC_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "plot_confusion_matrix(c, normalize=True, target_names=['Negative Review', 'Positive Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particular prediction with the GRU model\n",
    "model = create_gru(vocab_size, embedding_matrix)\n",
    "rvw = ['I laughed all the way through this rotten movie It so unbelievable woman leaves her husband after many years of marriage has breakdown in front of real estate office What happens The office manager comes outside and offers her job Hilarious Next thing you know the two women are going at it Yep they re lesbians Nothing rings true in this Lifetime for Women with nothing better to do movie Clunky dialogue like don want to spend the rest of my life feeling like had chance to be happy and didn take it doesn help There a wealthy distant mother who disapproves of her daughter new relationship sassy black maid unbelievable that in the year film gets made in which there a sassy black maid Hattie McDaniel must be turning in her grave The woman has husband who freaks out and wants custody of the snotty teenage kids Sheesh No cliche is left unturned']\n",
    "make_prediction(model, checkpoint_filepath_gru, rvw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_cnn(vocab_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=MAX_LEN , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(X_train, y_train, X_test, y_test, model):\n",
    "    model_checkpoint_callback_cnn = create_checkpoint(checkpoint_filepath_cnn)\n",
    "    # In each epoch the actual hyperparameters will be tested with the validation set\n",
    "    out_values = model.fit(X_train, y_train, batch_size=128, epochs=6,\n",
    "                           callbacks=[model_checkpoint_callback_cnn],\n",
    "                           verbose=1,\n",
    "                           validation_split=0.2)\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Test Score for the Convolutional Neural Network:\", score_test[0])\n",
    "    print(\"Test Accuracy for the Convolutional Neural Network:\", score_test[1])\n",
    "    print_graphs(out_values)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train cnn and print graphs\n",
    "model = create_model_cnn(vocab_size, embedding_matrix)\n",
    "print(model.summary())\n",
    "model = train_cnn(X_train, y_train, X_test, y_test, model)\n",
    "# Save the model \n",
    "model.save(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for the CNN model\n",
    "model = create_model_cnn(vocab_size, embedding_matrix)\n",
    "model.load_weights(checkpoint_filepath_cnn)\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "\n",
    "fpr, tpr, roc_auc, c = get_performance_metrics(y_test.reshape(-1, 1), predictions.astype('int'))\n",
    "print_ROC_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "plot_confusion_matrix(c, normalize=True, target_names=['Negative Review', 'Positive Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particular prediction with the CNN model\n",
    "model = create_model_cnn(vocab_size, embedding_matrix)\n",
    "rvw = ['I laughed all the way through this rotten movie It so unbelievable woman leaves her husband after many years of marriage has breakdown in front of real estate office What happens The office manager comes outside and offers her job Hilarious Next thing you know the two women are going at it Yep they re lesbians Nothing rings true in this Lifetime for Women with nothing better to do movie Clunky dialogue like don want to spend the rest of my life feeling like had chance to be happy and didn take it doesn help There a wealthy distant mother who disapproves of her daughter new relationship sassy black maid unbelievable that in the year film gets made in which there a sassy black maid Hattie McDaniel must be turning in her grave The woman has husband who freaks out and wants custody of the snotty teenage kids Sheesh No cliche is left unturned']\n",
    "make_prediction(model, checkpoint_filepath_cnn, rvw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lstm(vocab_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=MAX_LEN , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(X_train, y_train, X_test, y_test, model):\n",
    "    model_checkpoint_callback_lstm = create_checkpoint(checkpoint_filepath_lstm)\n",
    "    out_values = model.fit(X_train, y_train, batch_size=128, epochs=6,\n",
    "                           callbacks=[model_checkpoint_callback_lstm],\n",
    "                           verbose=1, validation_split=0.2)\n",
    "\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Test Score in the Recurrent Neural Network:\", score_test[0])\n",
    "    print(\"Test Accuracy in the Recurrent Neural Network:\", score_test[1])\n",
    "    print_graphs(out_values)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train lstm model and print graphs\n",
    "model = create_model_lstm(vocab_size, embedding_matrix)\n",
    "print(model.summary())\n",
    "model = train_lstm(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for the LSTM model\n",
    "model = create_model_lstm(vocab_size, embedding_matrix)\n",
    "model.load_weights(checkpoint_filepath_lstm)\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "\n",
    "fpr, tpr, roc_auc, c = get_performance_metrics(y_test.reshape(-1, 1), predictions.astype('int'))\n",
    "print_ROC_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "plot_confusion_matrix(c, normalize=True, target_names=['Negative Review', 'Positive Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particular prediction with the LSTM model\n",
    "model = create_model_lstm(vocab_size, embedding_matrix)\n",
    "rvw = ['I laughed all the way through this rotten movie It so unbelievable woman leaves her husband after many years of marriage has breakdown in front of real estate office What happens The office manager comes outside and offers her job Hilarious Next thing you know the two women are going at it Yep they re lesbians Nothing rings true in this Lifetime for Women with nothing better to do movie Clunky dialogue like don want to spend the rest of my life feeling like had chance to be happy and didn take it doesn help There a wealthy distant mother who disapproves of her daughter new relationship sassy black maid unbelievable that in the year film gets made in which there a sassy black maid Hattie McDaniel must be turning in her grave The woman has husband who freaks out and wants custody of the snotty teenage kids Sheesh No cliche is left unturned']\n",
    "make_prediction(model, checkpoint_filepath_lstm, rvw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bilateral LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved lstm model: We are going to try Bidirectional LSTM and \n",
    "def create_bilateral_lstm(vocab_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=MAX_LEN , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(units = 64,                             \n",
    "              return_sequences=True),\n",
    "              input_shape=(X_train.shape[0], X_train.shape[1])))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add((Dense(1)))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bilateral_lstm(X_train, y_train, X_test, y_test, model):\n",
    "    model_checkpoint_callback_bilstm = create_checkpoint(checkpoint_filepath_bi_lstm)\n",
    "    out_values = model.fit(X_train, y_train, batch_size=128, epochs=6,\n",
    "                           callbacks=[model_checkpoint_callback_bilstm],\n",
    "                           verbose=1, validation_split=0.2)\n",
    "\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Test Score for the Bilateral LSTM model:\", score_test[0])\n",
    "    print(\"Test Accuracy for the Bilateral LSTM model:\", score_test[1])\n",
    "    print_graphs(out_values)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bilateral model\n",
    "model = create_bilateral_lstm(vocab_size, embedding_matrix)\n",
    "print(model.summary())\n",
    "model = train_bilateral_lstm(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for the Bilateral LSTM model\n",
    "model = create_bilateral_lstm(vocab_size, embedding_matrix)\n",
    "model.load_weights(checkpoint_filepath_bi_lstm)\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "\n",
    "fpr, tpr, roc_auc, c = get_performance_metrics(y_test.reshape(-1, 1), predictions.astype('int'))\n",
    "print_ROC_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "plot_confusion_matrix(c, normalize=True, target_names=['Negative Review', 'Positive Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particular prediction with the Bilateral LSTM model\n",
    "model = create_bilateral_lstm(vocab_size, embedding_matrix)\n",
    "rvw = ['I laughed all the way through this rotten movie It so unbelievable woman leaves her husband after many years of marriage has breakdown in front of real estate office What happens The office manager comes outside and offers her job Hilarious Next thing you know the two women are going at it Yep they re lesbians Nothing rings true in this Lifetime for Women with nothing better to do movie Clunky dialogue like don want to spend the rest of my life feeling like had chance to be happy and didn take it doesn help There a wealthy distant mother who disapproves of her daughter new relationship sassy black maid unbelievable that in the year film gets made in which there a sassy black maid Hattie McDaniel must be turning in her grave The woman has husband who freaks out and wants custody of the snotty teenage kids Sheesh No cliche is left unturned']\n",
    "make_prediction(model, checkpoint_filepath_bi_lstm, rvw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Improved LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to try to improve the LSTM model by adding a SpatialDropout1D layer and other Dropout and Dense layers\n",
    "def create_impr_lstm(vocab_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=MAX_LEN , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SpatialDropout1D(0.25))\n",
    "    model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_impr_lstm(X_train, y_train, X_test, y_test, model):\n",
    "    model_checkpoint_callback_best_lstm = create_checkpoint(checkpoint_filepath_best_lstm)\n",
    "    out_values = model.fit(X_train, y_train, batch_size=128, epochs=6,\n",
    "                           callbacks=[model_checkpoint_callback_best_lstm],\n",
    "                           verbose=1, validation_split=0.2)\n",
    "\n",
    "    score_test = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Test Score for the best LSTM model:\", score_test[0])\n",
    "    print(\"Test Accuracy for the best LSTM model:\", score_test[1])\n",
    "    print_graphs(out_values)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the trying to improve the LSTM model\n",
    "model = create_impr_lstm(vocab_size, embedding_matrix)\n",
    "print(model.summary())\n",
    "model = train_impr_lstm(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for the improved LSTM model\n",
    "model = create_impr_lstm(vocab_size, embedding_matrix)\n",
    "model.load_weights(checkpoint_filepath_best_lstm)\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "\n",
    "fpr, tpr, roc_auc, c = get_performance_metrics(y_test.reshape(-1, 1), predictions.astype('int'))\n",
    "print_ROC_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "plot_confusion_matrix(c, normalize=True, target_names=['Negative Review', 'Positive Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particular prediction with the improved model\n",
    "model = create_impr_lstm(vocab_size, embedding_matrix)\n",
    "rvw = ['I laughed all the way through this rotten movie It so unbelievable woman leaves her husband after many years of marriage has breakdown in front of real estate office What happens The office manager comes outside and offers her job Hilarious Next thing you know the two women are going at it Yep they re lesbians Nothing rings true in this Lifetime for Women with nothing better to do movie Clunky dialogue like don want to spend the rest of my life feeling like had chance to be happy and didn take it doesn help There a wealthy distant mother who disapproves of her daughter new relationship sassy black maid unbelievable that in the year film gets made in which there a sassy black maid Hattie McDaniel must be turning in her grave The woman has husband who freaks out and wants custody of the snotty teenage kids Sheesh No cliche is left unturned']\n",
    "make_prediction(model, checkpoint_filepath_best_lstm, rvw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
